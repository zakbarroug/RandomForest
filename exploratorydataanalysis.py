# -*- coding: utf-8 -*-
"""ExploratoryDataAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18V2LVEhkZkZdhjNDKo0Jo30RgYRApoyP

Étape 1 : Exploration des données (EDA)

Examine la structure des fichiers (données de vente, clients, interactions, etc.).

Identifier les types de données : quantitatives (chiffres de vente), qualitatives (catégories de clients), temporelles (dates de transaction).

Aperçu des Datasets
Accounts Dataset:

Contient des informations sur les comptes clients, y compris leur secteur, l'année d'établissement, les revenus, le nombre d'employés, etc.
Colonnes intéressantes : account, sector, year_established, revenue, employees.
Points à explorer :
Les revenus (revenue) et le nombre d'employés (employees) semblent être des variables clés pour mesurer la taille des entreprises.
Data Dictionary Dataset:

Fournit des descriptions pour chaque champ des autres datasets.
Peut être utilisé comme référence pour comprendre les colonnes et leurs significations.
Products Dataset:

Contient des informations sur les produits, avec leur nom, série, et prix de vente.
Colonnes importantes : product, sales_price.
Points à explorer :
Les séries des produits (series) peuvent être utilisées pour segmenter les ventes et comprendre la distribution des produits.
Sales Pipeline Dataset:

Contient des informations sur les opportunités de vente, y compris les agents de vente, le produit, le compte, le stade de la vente (deal_stage), et la valeur de clôture (close_value).
Colonnes clés : opportunity_id, sales_agent, product, account, deal_stage, close_value.
Points à explorer :
Les dates d'engagement et de clôture (engage_date, close_date) permettront de calculer la durée des cycles de vente.
Analyser le deal_stage pour identifier les goulots d'étranglement.
Sales Teams Dataset:

Fournit des informations sur les agents de vente et leurs managers, ainsi que leur bureau régional.
Colonnes importantes : sales_agent, manager, regional_office.
Points à explorer :
Cela peut être utile pour évaluer les performances des équipes par bureau ou par manager.

Pour commencer, on va charger chaque dataset et obtenir un aperçu initial.

Utilise pandas pour charger les fichiers et explorer leur contenu.
"""

import pandas as pd

import pandas as pd

# Charger les datasets
accounts_df = pd.read_csv('accounts.csv')
products_df = pd.read_csv('products.csv')
sales_pipeline_df = pd.read_csv('sales_pipeline.csv')
sales_teams_df = pd.read_csv('sales_teams.csv')
data_dictionary_df = pd.read_csv('data_dictionary.csv')

# Afficher les premières lignes de chaque dataset
print("Accounts Dataset:")
print(accounts_df.head())
print("\nProducts Dataset:")
print(products_df.head())
print("\nSales Pipeline Dataset:")
print(sales_pipeline_df.head())
print("\nSales Teams Dataset:")
print(sales_teams_df.head())
print("\nData Dictionary Dataset:")
print(data_dictionary_df.head())

""" utilisation .info() pour obtenir des informations sur les types de données et les valeurs manquantes."""

# Informations sur les datasets
accounts_df.info()
products_df.info()
sales_pipeline_df.info()
sales_teams_df.info()
data_dictionary_df.info()

"""est ressorti du .info sur les datasets :

Gestion des valeurs manquantes :

Pour subsidiary_of dans accounts_df, il faudra décider si les valeurs manquantes sont pertinentes à remplir (par exemple par "Non applicable") ou si elles peuvent être ignorées.
Pour sales_pipeline_df, les colonnes engage_date et close_date doivent être converties en datetime, puis les valeurs manquantes devront être traitées : par exemple, close_date est manquante si l'opportunité est encore ouverte.
Analyse des types de données :

Assure-toi de convertir les colonnes de dates en datetime pour les manipuler correctement. Ceci permettra de calculer des statistiques comme la durée des cycles de vente.
Vérification de la pertinence des données :

Pour sales_pipeline_df, il serait utile de vérifier les relations entre deal_stage et les valeurs manquantes de close_date pour s'assurer qu'elles correspondent (par exemple, si l'opportunité n'est pas fermée, il est normal que close_date soit vide).

!!!! on va prendre en compte toutes ses infos pour corrige cela dans la partie nettoyage

Pour comprendre les statistiques de base (moyenne, médiane, minimum, maximum), utilise .describe() :
"""

# Statistiques descriptives des datasets
print(accounts_df.describe())
print(products_df.describe())
print(sales_pipeline_df.describe())
print(sales_teams_df.describe())

"""interpretation pour chaque dataset :
Accounts Dataset
Valeurs Manquantes :
La colonne subsidiary_of ne contient que 15 valeurs sur 85, ce qui signifie que beaucoup de comptes n'ont pas de société mère indiquée. Les valeurs manquantes sont normales si ces comptes ne font pas partie d'une société mère.
Type de Données :
Les types de données sont adaptés : les revenus (revenue) sont des float64, le nombre d'employés (employees) est int64, et les noms sont des object.
Statistiques Descriptives :
La moyenne des revenus est 1994.63 millions, et la valeur maximale est 11698.03 millions.
L'année d'établissement des comptes varie entre 1979 et 2017, avec une médiane en 1996.
Les employés varient de 9 à 34288, ce qui montre une grande disparité entre les tailles des entreprises.
Products Dataset
Valeurs Manquantes :
Il n'y a pas de valeurs manquantes.
Type de Données :
Les prix de vente (sales_price) sont des int64, ce qui est correct.
Statistiques Descriptives :
Le prix des produits varie de 55 à 26768, avec une valeur médiane à 3393. Cela indique des écarts importants entre les différents produits.
Sales Pipeline Dataset
Valeurs Manquantes :
account a des valeurs manquantes (7375 non-null sur 8800), ce qui signifie que certaines opportunités ne sont pas reliées à un compte client.
engage_date et close_date ont des valeurs manquantes, indiquant des opportunités sans engagement ou sans clôture. close_date a beaucoup de valeurs manquantes, ce qui est probablement dû au fait que beaucoup d'opportunités ne sont pas encore fermées.
Type de Données :
Les dates (engage_date, close_date) sont actuellement de type object. Elles doivent être converties en datetime pour faciliter les calculs de durée.
Statistiques Descriptives :
La valeur de clôture (close_value) varie de 0 à 30288, avec une médiane à 472. Cela montre une distribution très asymétrique des valeurs de vente, beaucoup d'opportunités étant de faible valeur.
Sales Teams Dataset
Valeurs Manquantes :
Pas de valeurs manquantes.
Type de Données :
Tous les types de données sont object, ce qui est approprié car il s'agit de noms et de régions.
Informations Supplémentaires :
Il y a 35 agents de vente et 6 managers. Le bureau régional le plus fréquent est le bureau East.
Data Dictionary Dataset
Valeurs Manquantes :
Pas de valeurs manquantes.
Type de Données :
Tout est de type object, ce qui est correct.
Conclusion et Prochaines Étapes
Gestion des valeurs manquantes :
Pour accounts_df, remplir subsidiary_of avec "Non applicable" ou analyser les comptes sans société mère.
Pour sales_pipeline_df, convertir engage_date et close_date en datetime. Gérer les valeurs manquantes en fonction du contexte (ex. : close_date manquant signifie que l'opportunité est ouverte).
Visualisations et Analyses :
Les visualisations devraient se concentrer sur la répartition des revenus des comptes, la variation des prix des produits, et les valeurs de clôture des opportunités.
"""

# Charger les fichiers CSV dans le même dossier que le notebook
accounts_df = pd.read_csv('accounts.csv')
products_df = pd.read_csv('products.csv')
sales_pipeline_df = pd.read_csv('sales_pipeline.csv')
sales_teams_df = pd.read_csv('sales_teams.csv')
data_dictionary_df = pd.read_csv('data_dictionary.csv')

# Aperçu des premières lignes de chaque dataset
print("Accounts Dataset:")
print(accounts_df.head())
print("\nProducts Dataset:")
print(products_df.head())
print("\nSales Pipeline Dataset:")
print(sales_pipeline_df.head())
print("\nSales Teams Dataset:")
print(sales_teams_df.head())
print("\nData Dictionary Dataset:")
print(data_dictionary_df.head())

# Informations sur chaque dataset (valeurs manquantes, types de données, etc.)
print("\nInfos Accounts Dataset:")
print(accounts_df.info())
print("\nInfos Products Dataset:")
print(products_df.info())
print("\nInfos Sales Pipeline Dataset:")
print(sales_pipeline_df.info())
print("\nInfos Sales Teams Dataset:")
print(sales_teams_df.info())

# Statistiques descriptives des colonnes numériques
print("\nStatistiques descriptives des datasets")
print("Accounts Dataset:")
print(accounts_df.describe())
print("\nProducts Dataset:")
print(products_df.describe())
print("\nSales Pipeline Dataset:")
print(sales_pipeline_df.describe())
print("\nSales Teams Dataset:")
print(sales_teams_df.describe())

# Vérifier les valeurs manquantes
print("\nValeurs manquantes par colonne dans Accounts Dataset:")
print(accounts_df.isnull().sum())
print("\nValeurs manquantes par colonne dans Products Dataset:")
print(products_df.isnull().sum())
print("\nValeurs manquantes par colonne dans Sales Pipeline Dataset:")
print(sales_pipeline_df.isnull().sum())
print("\nValeurs manquantes par colonne dans Sales Teams Dataset:")
print(sales_teams_df.isnull().sum())

"""Etape netoyyage code sur une base plus saine"""

# Supprimer la colonne 'subsidiary_of' car elle ne sera pas nécessaire
accounts_df.drop(columns=['subsidiary_of'], inplace=True)

# Remplacer les valeurs manquantes
sales_pipeline_df['account'].fillna('Non applicable', inplace=True)
sales_pipeline_df['engage_date'] = pd.to_datetime(sales_pipeline_df['engage_date'], errors='coerce')
sales_pipeline_df['close_date'] = pd.to_datetime(sales_pipeline_df['close_date'], errors='coerce')
sales_pipeline_df['engage_date'].fillna(sales_pipeline_df['engage_date'].mode()[0], inplace=True)
sales_pipeline_df['close_date'].fillna(sales_pipeline_df['close_date'].mode()[0], inplace=True)
sales_pipeline_df['close_value'].fillna(sales_pipeline_df['close_value'].median(), inplace=True)

# Normaliser les colonnes 'product' pour éviter les erreurs lors des jointures
products_df['product'] = products_df['product'].str.upper().str.replace(' ', '').str.strip()
sales_pipeline_df['product'] = sales_pipeline_df['product'].str.upper().str.replace(' ', '').str.strip()

# Vérifier les valeurs uniques dans chaque dataset après normalisation
print("Valeurs uniques dans products_df['product'] :")
print(products_df['product'].unique())
print("\nValeurs uniques dans sales_pipeline_df['product'] :")
print(sales_pipeline_df['product'].unique())

# Vérifier les valeurs manquantes dans chaque dataset
print("Valeurs manquantes par colonne dans Accounts Dataset:")
print(accounts_df.isnull().sum())

print("\nValeurs manquantes par colonne dans Products Dataset:")
print(products_df.isnull().sum())

print("\nValeurs manquantes par colonne dans Sales Pipeline Dataset:")
print(sales_pipeline_df.isnull().sum())

print("\nValeurs manquantes par colonne dans Sales Teams Dataset:")
print(sales_teams_df.isnull().sum())

"""au dessus pârfait aucune colonne nul
maintenant en dessous verifie des format date
"""

print(sales_pipeline_df.info())

"""enregistrement des nouveaux dataset nettoyer et modifie"""

# Enregistrer les datasets nettoyés
accounts_df.to_csv('accounts_cleaned.csv', index=False)
products_df.to_csv('products_cleaned.csv', index=False)
sales_pipeline_df.to_csv('sales_pipeline_cleaned.csv', index=False)
sales_teams_df.to_csv('sales_teams_cleaned.csv', index=False)
data_dictionary_df.to_csv('data_dictionary_cleaned.csv', index=False)

"""Modélisation des Données et Machine Learning

champs sur lequels nous allons travailler
-Prédiction des ventes futures : Utiliser les données historiques de vente pour prévoir les revenus futurs par produit ou par région.
-Évaluation de la performance des agents : Analyser les caractéristiques des agents pour identifier ceux qui sont susceptibles de performer le mieux, ou pour prédire les performances futures en termes de ventes.
-Produits prometteurs : Identifier les caractéristiques des produits les plus performants pour prédire lesquels seront les plus rentables dans le futur.

verif avant ml
"""

# Commençons par la première étape : Gérer les valeurs manquantes dans les datasets

import pandas as pd

# Charger les datasets depuis les fichiers que nous avons
accounts_df = pd.read_csv('accounts_cleaned.csv')
products_df = pd.read_csv('products_cleaned.csv')
sales_pipeline_df = pd.read_csv('sales_pipeline_cleaned.csv')
sales_teams_df = pd.read_csv('sales_teams_cleaned.csv')
data_dictionary_df = pd.read_csv('data_dictionary_cleaned.csv')

# Étape 1: Valeurs Manquantes - Examinons d'abord les valeurs manquantes
accounts_missing = accounts_df.isnull().sum()
products_missing = products_df.isnull().sum()
sales_pipeline_missing = sales_pipeline_df.isnull().sum()
sales_teams_missing = sales_teams_df.isnull().sum()

# Afficher les colonnes avec des valeurs manquantes pour chaque dataset
missing_values_report = {
    "Accounts Dataset": accounts_missing[accounts_missing > 0],
    "Products Dataset": products_missing[products_missing > 0],
    "Sales Pipeline Dataset": sales_pipeline_missing[sales_pipeline_missing > 0],
    "Sales Teams Dataset": sales_teams_missing[sales_teams_missing > 0]
}

missing_values_report

!python -m pip install --upgrade pip

!pip install numpy scikit-learn

import numpy as np
import sklearn

"""CI DESSOUS
nous avons un probleme avec certaines colonnes categorielle , nous allons donc les one hot encode et les normalise egalement pour pas que cela ne pose de probleme lors des lancement d apprentissage
"""

from sklearn.preprocessing import OneHotEncoder, StandardScaler
import numpy as np

# Étape 2 : Encodage des Variables Catégorielles
# Nous allons utiliser OneHotEncoder pour encoder les variables catégorielles

# Créer une copie des datasets originaux pour éviter d'écraser les données initiales
accounts_df_encoded = accounts_df.copy()
sales_pipeline_df_encoded = sales_pipeline_df.copy()
sales_teams_df_encoded = sales_teams_df.copy()

# Sélection des colonnes catégorielles à encoder
categorical_columns_accounts = ['sector']
categorical_columns_sales_pipeline = ['deal_stage', 'product']
categorical_columns_sales_teams = ['manager', 'regional_office']

# Instancier le OneHotEncoder avec le bon paramètre
encoder = OneHotEncoder(sparse_output=False, drop='first')

# Encoder les colonnes du dataset accounts
encoded_accounts = encoder.fit_transform(accounts_df_encoded[categorical_columns_accounts])
encoded_accounts_df = pd.DataFrame(encoded_accounts, columns=encoder.get_feature_names_out(categorical_columns_accounts))
accounts_df_encoded = pd.concat([accounts_df_encoded.drop(columns=categorical_columns_accounts), encoded_accounts_df], axis=1)

# Encoder les colonnes du dataset sales_pipeline
encoded_sales_pipeline = encoder.fit_transform(sales_pipeline_df_encoded[categorical_columns_sales_pipeline])
encoded_sales_pipeline_df = pd.DataFrame(encoded_sales_pipeline, columns=encoder.get_feature_names_out(categorical_columns_sales_pipeline))
sales_pipeline_df_encoded = pd.concat([sales_pipeline_df_encoded.drop(columns=categorical_columns_sales_pipeline), encoded_sales_pipeline_df], axis=1)

# Encoder les colonnes du dataset sales_teams
encoded_sales_teams = encoder.fit_transform(sales_teams_df_encoded[categorical_columns_sales_teams])
encoded_sales_teams_df = pd.DataFrame(encoded_sales_teams, columns=encoder.get_feature_names_out(categorical_columns_sales_teams))
sales_teams_df_encoded = pd.concat([sales_teams_df_encoded.drop(columns=categorical_columns_sales_teams), encoded_sales_teams_df], axis=1)

# Étape 3 : Normalisation/Standardisation des Variables Numériques
# Nous allons utiliser StandardScaler pour standardiser les variables numériques

# Colonnes numériques à standardiser
numeric_columns_accounts = ['revenue', 'employees']
numeric_columns_sales_pipeline = ['close_value']

# Standardiser les colonnes du dataset accounts
scaler = StandardScaler()
scaled_accounts = scaler.fit_transform(accounts_df_encoded[numeric_columns_accounts])
scaled_accounts_df = pd.DataFrame(scaled_accounts, columns=numeric_columns_accounts)
accounts_df_encoded.update(scaled_accounts_df)

# Standardiser les colonnes du dataset sales_pipeline
scaled_sales_pipeline = scaler.fit_transform(sales_pipeline_df_encoded[numeric_columns_sales_pipeline])
scaled_sales_pipeline_df = pd.DataFrame(scaled_sales_pipeline, columns=numeric_columns_sales_pipeline)
sales_pipeline_df_encoded.update(scaled_sales_pipeline_df)

# Afficher les premières lignes des datasets transformés pour vérifier les encodages et la normalisation
print(accounts_df_encoded.head())
print(sales_pipeline_df_encoded.head())
print(sales_teams_df_encoded.head())

"""cree et enregistre les fichiers traiter ci dessus"""

# Enregistrer les datasets encodés et normalisés
accounts_df_encoded.to_csv('accounts_encoded_normalized.csv', index=False)
sales_pipeline_df_encoded.to_csv('sales_pipeline_encoded_normalized.csv', index=False)
sales_teams_df_encoded.to_csv('sales_teams_encoded_normalized.csv', index=False)

"""ci dessous on va creer un dataset final utilisable pour le ml , cela sera plus optimal pour le calcul et eviter les erreur mais egalement les appel reccurant de diffrent dtaset qui peuvent creer des probleme"""

# Charger les versions encodées et normalisées des datasets
accounts_encoded_normalized_df = pd.read_csv('accounts_encoded_normalized.csv')
sales_pipeline_encoded_normalized_df = pd.read_csv('sales_pipeline_encoded_normalized.csv')
sales_teams_encoded_normalized_df = pd.read_csv('sales_teams_encoded_normalized.csv')
products_cleaned_df = pd.read_csv('products_cleaned.csv')  # Pour products, nous utilisons la version nettoyée, car il n'y avait pas d'encodage spécifique mentionné.

# Vérifier les colonnes présentes dans chaque dataset avant la fusion
print("Colonnes dans sales_pipeline_encoded_normalized_df:", sales_pipeline_encoded_normalized_df.columns)
print("Colonnes dans products_cleaned_df:", products_cleaned_df.columns)
print("Colonnes dans accounts_encoded_normalized_df:", accounts_encoded_normalized_df.columns)

# Charger les versions encodées et normalisées des datasets
accounts_encoded_normalized_df = pd.read_csv('accounts_encoded_normalized.csv')
sales_pipeline_encoded_normalized_df = pd.read_csv('sales_pipeline_encoded_normalized.csv')
sales_teams_encoded_normalized_df = pd.read_csv('sales_teams_encoded_normalized.csv')

# Vérifier les colonnes présentes dans chaque dataset avant la fusion
print("Colonnes dans sales_pipeline_encoded_normalized_df:", sales_pipeline_encoded_normalized_df.columns)
print("Colonnes dans accounts_encoded_normalized_df:", accounts_encoded_normalized_df.columns)

# Étape 1 : Fusionner sales_pipeline_encoded_normalized_df avec accounts_encoded_normalized_df
merged_df = sales_pipeline_encoded_normalized_df.merge(accounts_encoded_normalized_df, on='account', how='left')

# Étape 2 : Fusionner avec sales_teams_encoded_normalized_df sur 'sales_agent'
if 'sales_agent' in merged_df.columns and 'sales_agent' in sales_teams_encoded_normalized_df.columns:
    merged_df = merged_df.merge(sales_teams_encoded_normalized_df, on='sales_agent', how='left')
else:
    print("La colonne 'sales_agent' n'est pas présente dans l'un des DataFrames, veuillez vérifier les noms de colonnes.")

# Afficher les premières lignes du DataFrame fusionné
print("Aperçu du DataFrame fusionné :")
print(merged_df.head())

# Enregistrer le DataFrame fusionné
merged_df.to_csv('merged_sales_data_encoded_normalized.csv', index=False)
print("Le DataFrame fusionné a été enregistré sous 'merged_sales_data_encoded_normalized.csv'.")

"""Résumé de l'Étape de Fusion :
Nous avons fusionné les datasets :
sales_pipeline_encoded_normalized_df avec accounts_encoded_normalized_df et sales_teams_encoded_normalized_df.
Le DataFrame final contient des colonnes telles que :
Les caractéristiques des ventes (opportunity_id, sales_agent, close_value, etc.).
Les informations des comptes clients (sector_entertainment, sector_medical, etc.).
Les caractéristiques encodées des produits (product_GTXBASIC, product_MGSPECIAL, etc.), ce qui rend la fusion avec products_cleaned_df redondante.
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Charger le DataFrame fusionné à partir du fichier téléchargé
file_path = 'merged_sales_data_encoded_normalized.csv'
merged_df = pd.read_csv(file_path)

# Vérifier l'intégrité du DataFrame fusionné
print("Aperçu des premières lignes du DataFrame fusionné :")
print(merged_df.head())

print("\nInformations sur le DataFrame :")
print(merged_df.info())

print("\nStatistiques descriptives des colonnes numériques :")
print(merged_df.describe())

# Étape 1 : Définir l'objectif du modèle
# Nous allons prédire la valeur de clôture (`close_value`) des ventes.

# Sélection des features et de la cible (target)
# Features : Toutes les colonnes à l'exception de `close_value` et des colonnes inutiles (par exemple, les identifiants textuels)
features = merged_df.drop(columns=['close_value', 'opportunity_id', 'engage_date', 'close_date', 'account', 'sales_agent'])
target = merged_df['close_value']

# Vérification rapide des features et de la target
print("\nAperçu des Features :")
print(features.head())

print("\nAperçu de la Target :")
print(target.head())

# Étape suivante : Séparer les données en ensemble d'entraînement et ensemble de test
# Séparer les données en 80% d'entraînement et 20% de test
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Vérifier la taille des jeux de données
print("\nTaille de l'ensemble d'entraînement :", X_train.shape)
print("Taille de l'ensemble de test :", X_test.shape)

# Afficher les premières lignes de l'ensemble d'entraînement pour confirmer
print("\nAperçu des données d'entraînement :")
print(X_train.head())

"""ci dessus

Ce Que Nous Avons :
Aperçu des Données : Nous avons visualisé les premières lignes du DataFrame fusionné. Tout semble être en ordre, les colonnes ont été correctement encodées et normalisées, et les caractéristiques (features) ainsi que la cible (target) sont prêtes.
Séparation en Données d'Entraînement et de Test :
Ensemble d'entraînement : 7 040 exemples.
Ensemble de test : 1 760 exemples.
Les données sont maintenant prêtes pour entraîner un modèle de Machine Learning

Pour l'étape suivante, nous allons choisir un modèle simple pour prédire la valeur de clôture (close_value) des ventes REGRESSION LIENAIRE

ja i en probleme encore des des colonne categorielle qu ej avais oublie d encore ci dessous je vais regle cela
"""

# Appliquer OneHotEncoder pour les colonnes catégorielles restantes sans spécifier de paramètre supplémentaire
encoder = OneHotEncoder(drop='first')
encoded_features = pd.DataFrame(encoder.fit_transform(features[categorical_columns]).toarray())

# Nommer les nouvelles colonnes encodées
encoded_features.columns = encoder.get_feature_names_out(categorical_columns)

# Supprimer les anciennes colonnes catégorielles et ajouter les nouvelles colonnes encodées
features_encoded = features.drop(columns=categorical_columns).reset_index(drop=True)
features_encoded = pd.concat([features_encoded, encoded_features], axis=1)

# Imputer les valeurs manquantes avec la médiane
imputer = SimpleImputer(strategy='median')
features_imputed = pd.DataFrame(imputer.fit_transform(features_encoded), columns=features_encoded.columns)

# Vérification après imputation
print("\nAperçu des features après imputation des valeurs manquantes :")
print(features_imputed.head())

# Refaire la séparation en ensemble d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.2, random_state=42)

# Réentraîner le modèle de régression linéaire
linear_regressor = LinearRegression()
linear_regressor.fit(X_train, y_train)

# Prédire les valeurs de l'ensemble de test
y_pred = linear_regressor.predict(X_test)

# Calculer les métriques de performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Afficher les résultats
print("\nPerformance du modèle de régression linéaire après imputation :")
print(f"Erreur quadratique moyenne (MSE) : {mse:.2f}")
print(f"Coefficient de détermination (R^2) : {r2:.2f}")

# Comparer les prédictions aux valeurs réelles pour les premières lignes
comparison_df = pd.DataFrame({'Valeurs réelles': y_test.values, 'Valeurs prédites': y_pred})
print("\nComparaison des valeurs réelles et prédites :")
print(comparison_df.head())

"""Résultats du Modèle de Régression Linéaire :
Performance du Modèle :

Erreur Quadratique Moyenne (MSE) : 0.36. Cette valeur indique l'erreur moyenne au carré entre les valeurs réelles et les valeurs prédites. Une valeur plus faible est souhaitable.
Coefficient de Détermination (R²) : 0.61. Cela signifie que 61% de la variance de la variable cible (close_value) est expliquée par les caractéristiques d'entrée du modèle.
Comparaison des Valeurs Réelles et Prédites :

Pour les premières lignes de l'ensemble de test, les prédictions du modèle ne sont pas parfaites, mais elles suivent une certaine tendance par rapport aux valeurs réelles.
Analyse des Résultats :
Un R² de 0.61 indique que le modèle a une certaine capacité de prédiction, mais il reste encore des marges de progression, probablement parce que la relation entre les caractéristiques et la cible est complexe ou non linéaire.
L'erreur (MSE) est présente, ce qui montre qu'il y a encore des différences entre les prédictions et les valeurs réelles.

Conclusion , bonne base mais peut etre encore ameliore , donc nous allons tester des modele plus avance
"""

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Créer une fonction pour entraîner et évaluer les modèles
def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    # Entraîner le modèle
    model.fit(X_train, y_train)

    # Prédire les valeurs sur l'ensemble de test
    y_pred = model.predict(X_test)

    # Calculer les métriques de performance
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Afficher les résultats
    print(f"\nPerformance du modèle {model_name} :")
    print(f"Erreur quadratique moyenne (MSE) : {mse:.2f}")
    print(f"Coefficient de détermination (R^2) : {r2:.2f}")

    # Comparer les prédictions aux valeurs réelles pour les premières lignes
    comparison_df = pd.DataFrame({'Valeurs réelles': y_test.values, 'Valeurs prédites': y_pred})
    print(f"\nComparaison des valeurs réelles et prédites pour {model_name} :")
    print(comparison_df.head())

# Étape 1 : Random Forest Regressor
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)
train_and_evaluate_model(random_forest_model, X_train, X_test, y_train, y_test, "Random Forest Regressor")

# Étape 2 : Gradient Boosting Regressor
gradient_boosting_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
train_and_evaluate_model(gradient_boosting_model, X_train, X_test, y_train, y_test, "Gradient Boosting Regressor")

"""Voici les résultats des deux modèles avancés que nous avons testés : Random Forest Regressor et Gradient Boosting Regressor.

Résumé des Performances :
Random Forest Regressor :

Erreur Quadratique Moyenne (MSE) : 0.02 (très faible, ce qui est bon signe).
Coefficient de Détermination (R²) : 0.98, ce qui montre que 98% de la variance de la cible est expliquée par le modèle.
Comparaison des Valeurs : Les prédictions sont très proches des valeurs réelles, indiquant une très bonne capacité de prédiction.
Gradient Boosting Regressor :

Erreur Quadratique Moyenne (MSE) : 0.16 (plus élevée que pour la forêt aléatoire, mais toujours acceptable).
Coefficient de Détermination (R²) : 0.83, ce qui montre que le modèle explique 83% de la variance des données.
Comparaison des Valeurs : Les prédictions sont proches des valeurs réelles, mais un peu moins précises que celles du modèle Random Forest.
Interprétation :
Random Forest Regressor est le modèle le plus performant ici, avec un R² de 0.98, ce qui est excellent. Il capture très bien les relations dans les données, avec une faible erreur.
Gradient Boosting Regressor est également performant, mais il semble un peu moins précis, avec un R² de 0.83.

clairement le random forest est super bon , on choisie donc celui la

!!!!!!!!!!!!!!!!!!!!!! explication complete !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Je comprends parfaitement, et c'est une question très importante, surtout lorsqu'il s'agit de **comprendre la valeur ajoutée** d'un modèle de Machine Learning pour un projet concret. L'objectif est de comprendre ce que le modèle fait, ce qu'il prédit, et comment cela peut être utilisé pour **prendre des décisions stratégiques**.

### **Résumé du Modèle Random Forest et de Sa Valeur Ajoutée**

#### **1. Ce Que Nous Avons Fait :**
- Nous avons entraîné un **modèle de régression** basé sur les données CRM de l'entreprise.
- Le modèle que nous avons choisi est un **Random Forest Regressor**, qui est bien adapté pour capturer des relations complexes entre les caractéristiques des clients et des transactions, et la valeur que l'on veut prédire.

#### **2. Ce Que Prédit le Modèle Random Forest :**
- Le modèle prédit **la valeur de clôture des ventes** (`close_value`).
- **Concrètement**, la valeur de clôture représente le montant final que l'on pourrait obtenir lorsque l'opportunité commerciale est conclue.
- Chaque observation dans notre jeu de données représente une opportunité de vente (potentiellement avec un client). Le modèle estime à **combien cette opportunité pourrait être conclue**, en se basant sur les caractéristiques que nous lui avons fournies.

#### **3. Les Variables Utilisées :**
- Le modèle utilise des **caractéristiques** comme :
  - **Le secteur d'activité** (`sector`) du compte client.
  - **L'âge de l'entreprise** (`year_established`).
  - **Les revenus annuels** (`revenue`) de l'entreprise.
  - **L'étape actuelle de l'opportunité** (`deal_stage`).
  - **Le produit concerné** (`product`).
  - **Le responsable des ventes** (`sales_agent`).
  - Et d'autres caractéristiques qui ont été encodées et normalisées.

Ces caractéristiques aident le modèle à estimer la **probabilité de succès** et la **valeur potentielle de la vente** pour chaque opportunité.

#### **4. Valeur Ajoutée de Ce Modèle :**
Le modèle apporte plusieurs **valeurs ajoutées concrètes** pour l'entreprise :

1. **Meilleure Prédiction des Revenus Futurs** :
   - En ayant une estimation précise de la valeur de clôture de chaque opportunité, le modèle permet aux responsables commerciaux de **prédire les revenus futurs** de manière plus fiable.
   - Cela aide les équipes de **gestion financière** à planifier les flux de trésorerie et les budgets.

2. **Priorisation des Opportunités** :
   - Les opportunités avec des valeurs de clôture élevées ou une forte probabilité de succès peuvent être **priorisées** par l'équipe de vente.
   - Cela permet de **concentrer les efforts** sur les opportunités les plus prometteuses, maximisant ainsi le rendement de l'équipe.

3. **Optimisation des Stratégies de Vente** :
   - En analysant les caractéristiques qui influencent le plus la valeur de clôture, le modèle peut **fournir des recommandations** sur les segments de clients à cibler.
   - Par exemple, si le modèle montre que certains secteurs d'activité ont tendance à générer des valeurs de clôture plus élevées, l'équipe commerciale peut concentrer ses efforts sur ces secteurs spécifiques.

4. **Ajustement des Pratiques de Vente** :
   - En analysant les prédictions, l'entreprise peut comprendre les **facteurs de succès** (comme l'expérience du responsable commercial, le type de produit, l'emplacement géographique, etc.).
   - Cela permet de **former les équipes de vente** pour qu'elles adoptent des pratiques qui maximisent les chances de conclure des ventes à forte valeur.

5. **Amélioration de la Précision des Prévisions** :
   - Au lieu de se baser sur des estimations subjectives ou des données historiques seules, le modèle permet d'avoir des **prévisions chiffrées, objectives et basées sur des données** en temps réel.

#### **5. Exemple Concret de Valeur Ajoutée :**
Imaginons que nous avons 100 opportunités en cours, et que le modèle Random Forest prédit pour chacune d'elles une valeur de clôture :
- Le responsable des ventes peut utiliser ces prévisions pour **se concentrer sur les 20 opportunités** qui représentent 80% des revenus totaux potentiels (selon la règle du 80/20).
- Cela permet de **gagner du temps** et de **maximiser le rendement** en se focalisant sur les opportunités les plus lucratives.

### **Résumé Final :**
- **Ce que fait le modèle Random Forest** : Il **prédit la valeur de clôture** d'une opportunité de vente.
- **Comment cela apporte de la valeur** : Cela permet de **mieux prévoir les revenus**, de **prioriser les efforts de vente**, et de **prendre des décisions stratégiques** basées sur les données.
- **La précision du modèle** (avec un R² de **0.98**) nous montre qu'il est **fiable** et qu'il capture bien les relations entre les caractéristiques des ventes et leur valeur.

!!!!!!Prochain ML ou l objectif va etre de predire correctement ou anticiper quelles opportunités ont de fortes chances d'être gagnées ou perdues.

pour cela nous allons tester Random Forest Classifier et Logistic Regression.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Étape 1 : Préparation des données pour la classification
# Nous allons prédire la colonne `deal_stage_Won` (qui est une colonne binaire : 1 pour Gagné, 0 pour Perdu)

# Définir les features et la cible (target) pour la classification
# Nous supprimons ici aussi les colonnes qui ne sont pas utiles à la prédiction (identifiants textuels et variables de date)
features_classification = merged_df.drop(columns=['deal_stage_Won', 'deal_stage_Lost', 'opportunity_id', 'engage_date', 'close_date', 'account', 'sales_agent'])
target_classification = merged_df['deal_stage_Won']

# Identifier les colonnes catégorielles restantes
categorical_columns_classification = features_classification.select_dtypes(include=['object']).columns
print("Colonnes catégorielles restantes à encoder :", list(categorical_columns_classification))

# Appliquer OneHotEncoder pour les colonnes catégorielles restantes
encoder = OneHotEncoder(drop='first')
encoded_features_classification = pd.DataFrame(encoder.fit_transform(features_classification[categorical_columns_classification]).toarray())

# Nommer les nouvelles colonnes encodées
encoded_features_classification.columns = encoder.get_feature_names_out(categorical_columns_classification)

# Supprimer les anciennes colonnes catégorielles et ajouter les nouvelles colonnes encodées
features_classification_encoded = features_classification.drop(columns=categorical_columns_classification).reset_index(drop=True)
features_classification_encoded = pd.concat([features_classification_encoded, encoded_features_classification], axis=1)

# Imputer les valeurs manquantes avec la médiane
imputer_classification = SimpleImputer(strategy='median')
features_classification_imputed = pd.DataFrame(imputer_classification.fit_transform(features_classification_encoded), columns=features_classification_encoded.columns)

# Séparation des données en ensemble d'entraînement et de test
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(features_classification_imputed, target_classification, test_size=0.2, random_state=42)

# Étape 2 : Entraînement des modèles de classification

# 1. Random Forest Classifier
random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest_classifier.fit(X_train_clf, y_train_clf)

# Prédiction avec le modèle Random Forest
y_pred_rf = random_forest_classifier.predict(X_test_clf)

# Évaluation du modèle Random Forest
print("\nPerformance du Random Forest Classifier :")
print(f"Exactitude (Accuracy) : {accuracy_score(y_test_clf, y_pred_rf):.2f}")
print("Classification Report :")
print(classification_report(y_test_clf, y_pred_rf))
print("Matrice de Confusion :")
print(confusion_matrix(y_test_clf, y_pred_rf))

# 2. Logistic Regression
logistic_regression_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_regression_model.fit(X_train_clf, y_train_clf)

# Prédiction avec le modèle Logistic Regression
y_pred_lr = logistic_regression_model.predict(X_test_clf)

# Évaluation du modèle Logistic Regression
print("\nPerformance du Logistic Regression :")
print(f"Exactitude (Accuracy) : {accuracy_score(y_test_clf, y_pred_lr):.2f}")
print("Classification Report :")
print(classification_report(y_test_clf, y_pred_lr))
print("Matrice de Confusion :")
print(confusion_matrix(y_test_clf, y_pred_lr))

"""resultat des deux exceptionnel comparativement au ancien modele et aux ancien notebook

Voici les résultats des deux modèles de **classification** que nous avons testés : **Random Forest Classifier** et **Logistic Regression**. Le but était de classifier les opportunités en **gagnées (1)** ou **perdues (0)**.

### **1. Résultats du Modèle Random Forest Classifier :**
- **Exactitude (Accuracy)** : **0.99** (soit 99%), ce qui est **excellent** et indique que le modèle a une très grande précision sur l'ensemble de test.
- **Classification Report** :
  - **Precision et Recall** (rappel) sont tous deux très proches de **1.0**, ce qui signifie que le modèle est très précis pour identifier les opportunités gagnées et perdues.
  - **F1-score** est également de **0.99**, montrant une bonne balance entre précision et rappel.
- **Matrice de Confusion** :
  - **888** opportunités perdues correctement classées.
  - **862** opportunités gagnées correctement classées.
  - **6 faux positifs** (opportunités incorrectement classées comme gagnées).
  - **4 faux négatifs** (opportunités gagnées non détectées).

### **2. Résultats du Modèle Logistic Regression :**
- **Exactitude (Accuracy)** : **0.96** (soit 96%), ce qui est toujours **très bon**, mais légèrement inférieur à celui du Random Forest.
- **Classification Report** :
  - **Precision** et **Recall** pour les deux classes (gagné et perdu) sont **très élevés**, mais légèrement inférieurs à ceux du Random Forest.
  - **F1-score** est de **0.96**, ce qui montre une très bonne performance générale.
- **Matrice de Confusion** :
  - **831** opportunités perdues correctement classées.
  - **851** opportunités gagnées correctement classées.
  - **63 faux positifs** (plus élevés par rapport au Random Forest).
  - **15 faux négatifs** (opportunités gagnées incorrectement classées).

### **Analyse des Résultats :**
- **Random Forest Classifier** est le modèle le plus performant ici, avec une **exactitude de 99%**. La matrice de confusion montre qu'il fait **très peu d'erreurs** et qu'il est fiable pour distinguer les opportunités gagnées des opportunités perdues.
- **Logistic Regression** a également bien fonctionné, mais son **exactitude de 96%** est légèrement inférieure à celle du Random Forest. Il fait plus d'erreurs, particulièrement dans la classification des opportunités perdues.

!!!!!!!!!!!!!!!!!!!!!!!!!!resume !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

### **Valeur Ajoutée de la Classification des Opportunités :**
Le modèle de classification permet d'**anticiper quelles opportunités ont de fortes chances d'être gagnées ou perdues**. Voici comment cela apporte de la valeur :
1. **Priorisation des Efforts de Vente** :
   - En sachant à l'avance quelles opportunités sont **susceptibles d'être gagnées**, les commerciaux peuvent se concentrer sur celles-ci pour maximiser leur taux de conversion.
2. **Optimisation des Ressources** :
   - Les **opportunités risquées** peuvent être traitées différemment (par exemple, en adaptant le discours commercial ou en assignant un responsable de vente plus expérimenté).
3. **Meilleure Planification des Ventes** :
   - Les responsables commerciaux peuvent **mieux estimer le chiffre d'affaires à venir** en se basant sur les opportunités prédictives gagnées/perdues.

### **Prochaines Étapes :**
- **Utiliser le Modèle Random Forest Classifier** pour classifier les nouvelles opportunités, et fournir des rapports à l'équipe commerciale sur les **opportunités prioritaires**.
- **Analyser les Variables Importantes** du modèle Random Forest, pour comprendre les facteurs qui influencent le plus les décisions de gain ou de perte.
- **Développer un Tableau de Bord** interactif pour visualiser les prédictions et aider les équipes commerciales à prendre des décisions informées.
"""